{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66f6928",
   "metadata": {},
   "source": [
    "# Predicting diffusive size factors of a rock sample\n",
    "This tutorial demonstrates how to use PoreSpy's `diffusive_size_factor_AI` and `diffusive_size_factor_DNS` functions to examine the application of the AI-based size factor model on a real rock sample. Due to the computational cost of the DNS method, only a subsection of the image is used in this examples. However, the method can be applied similarly on the entir image of the rock sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2059cbe1",
   "metadata": {},
   "source": [
    "**For this specific sample:** \n",
    "\n",
    "Binary image data was obtained from the digital rock portals for Leopard rock [sample](https://www.digitalrocksportal.org/projects/317/origin_data/1380/) with Voxel length of 2.25 um."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ac583",
   "metadata": {},
   "source": [
    "## Import libaries and the AI model path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a5535de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jeff\\anaconda3\\envs\\dev\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing defs: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mporespy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mps\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m  \u001b[38;5;66;03m# if there was error importing, please install the h5py package\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\h5py\\__init__.py:33\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mhdf5_version_tuple \u001b[38;5;241m!=\u001b[39m version\u001b[38;5;241m.\u001b[39mhdf5_built_version_tuple:\n\u001b[0;32m     36\u001b[0m     _warn((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5py is running against HDF5 \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m when it was built against \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis may cause problems\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     38\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mversion\u001b[38;5;241m.\u001b[39mhdf5_version_tuple),\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mversion\u001b[38;5;241m.\u001b[39mhdf5_built_version_tuple)\n\u001b[0;32m     40\u001b[0m     ))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dev\\lib\\site-packages\\h5py\\version.py:15\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m    Versioning module for h5py.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m namedtuple\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m h5 \u001b[38;5;28;01mas\u001b[39;00m _h5\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\n",
      "File \u001b[1;32mh5py\\h5.pyx:1\u001b[0m, in \u001b[0;36minit h5py.h5\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing defs: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "import porespy as ps\n",
    "import h5py  # if there was error importing, please install the h5py package\n",
    "import importlib\n",
    "import warnings\n",
    "import openpnm as op\n",
    "import porespy as ps\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "import tensorflow as tf\n",
    "ps.visualization.set_mpl_style()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0561c569",
   "metadata": {},
   "source": [
    "Ensure the existence of model path, and create one if non-existant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"sf-model-lib\"):\n",
    "    !git clone https://github.com/PMEAL/sf-model-lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594cbaf5",
   "metadata": {},
   "source": [
    "In the cell below we import the proper library and assign the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495935ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5tools = importlib.import_module(\"sf-model-lib.h5tools\")\n",
    "DIR_WEIGHTS = \"sf-model-lib/diffusion\"\n",
    "fname_in = [f\"{DIR_WEIGHTS}/model_weights_part{j}.h5\" for j in [0, 1]]\n",
    "\n",
    "#Identifying hdf5 files and merging them together, the result would be a unique file\n",
    "h5tools.combine(fname_in, fname_out=f\"{DIR_WEIGHTS}/model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f4daa5",
   "metadata": {},
   "source": [
    "Next, we import folder path are related libraries (these chould be installed before usage) and define the AI path for\n",
    "the training data the wieghts. The training data is used in the backend of the `diffusive_size_factor_AI` to rescale the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_AI = \"./sf-model-lib/diffusion\"\n",
    "path_train = os.path.join(path_AI, 'g_train_original.hdf5')\n",
    "path_weights = os.path.join(path_AI, 'model_weights.h5')\n",
    "g_train = h5py.File(path_train, 'r')['g_train'][()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacbea8a",
   "metadata": {},
   "source": [
    "## Reading image of the rock sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5590be",
   "metadata": {},
   "source": [
    "As the image of the rock smaple was large, only a subsection of the image is used in this tutorial for rapid demonstration purposes. We saved a subsection of the Leopard rock sample image of size $100^3$ in PoreSpy's `test/fixtures` folder, which is used for this tutorial. However, the steps to download, read, and slice the rock sample image are provided as markdown cells in the next section for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9567d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_size = 2.25e-6\n",
    "with open('../../../test/fixtures/image_Leopard_slice100.npy', 'rb') as f:\n",
    "    im = np.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85defeb",
   "metadata": {},
   "source": [
    "## Additional info: steps to create a subsection of the rock sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455bcf46",
   "metadata": {},
   "source": [
    "1) Downloading the image of the rock sample: The cell below creates and ensures the existence of the specific sample path (here rock sample Leopard folder).\n",
    "The image of the sample can be downloaded through a link as of the cell below \"url\" and calling `download_image` function, or added seperately as a file to the rock sample folder. If the file is downloaded separately, the last line of the cell should be commented to prevent re-download.\n",
    "\n",
    "**Note:** \n",
    "The downloadable link for a binary image in the digital rocks portal can be found by selecting action\n",
    "button next to the binary image, selecting download, right click on the download and copy the link. Please note due to the large size of the image the downloading may take a few minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab9622",
   "metadata": {},
   "source": [
    "```python\n",
    "path = 'rock_sample_Leopard' # this is the path folder for reading/saving the image data for this example\n",
    "name = 'image_Leopard.raw' # this is the name of the saved file\n",
    "if not os.path.isdir(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "file_name = path+'/'+name\n",
    "url = 'https://www.digitalrocksportal.org/projects/317/images/223481/download/'\n",
    "\n",
    "def download_image(filename, url):\n",
    "    download_command = f'wget {url} -O {filename}'\n",
    "    try:\n",
    "        subprocess.run(download_command.split(' '))\n",
    "    except FileNotFoundError:\n",
    "        raise InterruptedError(f'wget was not found. Please make sure it is installed on your system.')\n",
    "    return\n",
    "\n",
    "download_image(file_name, url)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb59c3",
   "metadata": {},
   "source": [
    "2) Reading the image as numpy array:\n",
    "After importing the input image, we need to define the dimensions of the sample, this information is usually gathered from the source of the image. Here, a rock sample of size [1000,1000,1000] was used. Next step is to convert the image data to numpy arrays to be compatible with PoreSpy's input image type for `diffusive_size_factor_AI` and `diffusive_size_factor_DNS`. Here, numpy.fromfile was used to read the image. More details on this function can be found [here](https://numpy.org/doc/1.21/reference/generated/numpy.fromfile.html#:~:text=numpy.fromfile%20%C2%B6%20numpy.fromfile%28file%2C%20dtype%3Dfloat%2C%20count%3D-%201%2C%20sep%3D%27%27%2C%20offset%3D0%2C,as%20well%20as%20parsing%20simply%20formatted%20text%20files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1ffc50",
   "metadata": {},
   "source": [
    "```python\n",
    "voxelsx = 1000\n",
    "voxelsy = 1000\n",
    "voxelsz = 1000\n",
    "voxel_size = 2.25e-6\n",
    "\n",
    "im = np.fromfile(file_name, dtype=\"<i1\")\n",
    "im = np.reshape(im, (voxelsx,voxelsy,voxelsz))\n",
    "\n",
    "# pore space must be labeled as True and solid phase as False\n",
    "pore_space = im == 0 # sometimes this may be 255 or some other value depending on the source of the image\n",
    "im[pore_space] = True\n",
    "im[~pore_space] = False\n",
    "print(ps.metrics.porosity(im))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3108d2",
   "metadata": {},
   "source": [
    "**Note:** The line `(ps.metrics.porosity(im))` is to check the porosity level to the information from the input source description. If there is a significant difference, the labels of the input image may need to be reveresd. e.g. you may need to switch False and True in the code above or replace 0 with a different value. You can check the current values in the loaded image using np.unique(im)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d97cd6",
   "metadata": {},
   "source": [
    "3) Slicing the image: In this stage we slice the image to a smaller subsection. This is to speed up the process of prediction with the AI approach and the DNS approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac671b3a",
   "metadata": {},
   "source": [
    "```python\n",
    "im = im[:100,:100,:100]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29469268",
   "metadata": {},
   "source": [
    "## Segmentation of the image\n",
    "`Snow2` function is part of the PoreSpy's opensource package, which extracts the pore network and conns from the image.\n",
    "We then extract the pore network of the porous medium image using PoreSpy's snow2 algorithm. snow2 returns the segmented image of the porous medium as well as extracted network data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c521e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow = ps.networks.snow2(im, boundary_width=0, parallelization=None, voxel_size = voxel_size)\n",
    "regions = snow.regions\n",
    "net = snow.network\n",
    "conns = net['throat.conns']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807b27d3",
   "metadata": {},
   "source": [
    "## Size factor prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323da346",
   "metadata": {},
   "source": [
    "Create the AI model and load the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5769839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ps.networks.create_model()\n",
    "# Giving the path weights to .load function\n",
    "model.load_weights(path_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c4913b",
   "metadata": {},
   "source": [
    "Now that we have the regions, model and image data, we can use it for prediction. Finally we intiate the AI prediction process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd7cff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ai = ps.networks.diffusive_size_factor_AI(\n",
    "    regions,\n",
    "    model=model,\n",
    "    g_train=g_train,\n",
    "    throat_conns=conns,\n",
    "    voxel_size = voxel_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251da161",
   "metadata": {},
   "source": [
    "Similarly we run the DNS(Direct Numerical Simulation) **Note:** This cell is often the longest to exute. Here for comparison purposes, we used `time` to calculate the runtime of this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c91ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.time()\n",
    "predicted_dns = ps.networks.diffusive_size_factor_DNS(regions,\n",
    "                                                    throat_conns=conns,\n",
    "                                                         voxel_size = voxel_size)\n",
    "executionTime_dns = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime_dns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2297379",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "Once the values are predicted, you can save them ih hdf5 format for later use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46c4a8",
   "metadata": {},
   "source": [
    "Finally we plot the Comparison between AI results and DNS predicted results. This helps us to understand the deviation between the two and also mesaure the accuracy and the correctness of the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b638b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = np.max([predicted_ai, predicted_dns])\n",
    "plt.figure(figsize=[8,8])\n",
    "plt.plot(predicted_ai, predicted_dns, '*', [0, max_val], [0, max_val], 'r')\n",
    "plt.title(r'$R^2$ = ' + str(np.round(r2_score(predicted_dns, predicted_ai), 2)));\n",
    "plt.xlabel('AI_based size factor values')\n",
    "plt.ylabel('DNS_based size factor values')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
