{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the input as an image type\n",
    "The cell below creates and ensures the existence of the specific sample path.\n",
    "The image of the sample can be downloaded through a link as of the cell below \"drp_url\", or added seperately as a file.\n",
    "\n",
    "**For this specific sample:** \n",
    "    - Binary image data obtained from the filtered grayscale image data. The grayscale image was segmented at threshold level 71 calculated using the IsoData algorithm. For more details, please refer to the related publications.\n",
    "Segmented Yes Voxel length (x, y, z) 2.25, 2.25, 2.25 um Link: https://www.digitalrocksportal.org/projects/317/origin_data/1380/\n",
    "\n",
    "Image Type 8-bit Width 1000 Height 1000 Number of Slices 1000 Byte Order little-endian\n",
    "\n",
    "we assummed voxel_size=1, and we can postprocess units of the predictions by multiplying the results by **[voxel_size]**. However, PoreSpy's functions can accept the value of voxel_size to prevent post-processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import numpy as np\n",
    "import porespy as ps\n",
    "import h5py  # if there was error importing, please install the h5py package\n",
    "import importlib\n",
    "import warnings\n",
    "import openpnm as op\n",
    "import porespy as ps\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the existence of model path, and create one if non-existant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(\"sf-model-lib\"):\n",
    "    !git clone https://github.com/PMEAL/sf-model-lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we the proper library and assign the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h5tools = importlib.import_module(\"sf-model-lib.h5tools\")\n",
    "DIR_WEIGHTS = \"sf-model-lib/diffusion\"\n",
    "fname_in = [f\"{DIR_WEIGHTS}/model_weights_part{j}.h5\" for j in [0, 1]]\n",
    "\n",
    "#Identifying hdf5 files and merging them together, the result would be a unique file\n",
    "h5tools.combine(fname_in, fname_out=f\"{DIR_WEIGHTS}/model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below, we import are related libraries (these chould be installed before usage) \n",
    "Then we start the training of the model and load the wieghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ps.visualization.set_mpl_style()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "path_AI = \"./sf-model-lib/diffusion\"\n",
    "path_train = os.path.join(path_AI, 'g_train_original.hdf5')\n",
    "path_weights = os.path.join(path_AI, 'model_weights.h5')\n",
    "g_train = h5py.File(path_train, 'r')['g_train'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save the binary image as hdf5\n",
    "# TODO: double checking the names\n",
    "path='rock_sample_Leopard'# this is the path folder for saving the data for this example\n",
    "if not os.path.isdir(path):\n",
    "    os.makedirs(path)\n",
    "def download_geometry(filename, url):\n",
    "\n",
    "    download_command = f'wget {url} -O {filename}'\n",
    "    try:\n",
    "        subprocess.run(download_command.split(' '))\n",
    "    except FileNotFoundError:\n",
    "        raise InterruptedError(f'wget was not found. Please make sure it is installed on your system.')\n",
    "    return\n",
    "drp_url = 'https://www.digitalrocksportal.org/projects/317/images/223481/download/' # I found this link from selecting action\n",
    "# button next to the image, download and right click>copy link.\n",
    "file_name = path+'/image_Leopard.raw'\n",
    "#download_geometry(file_name, drp_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convertion to Numpy array\n",
    "After importing the input image, we need to define the voxels(dimensions of the sample),\n",
    "this information is usually gathered from the source of the image.\n",
    "Next step is to convert the image data to numpy arrays for easier mathematical computation.\n",
    "the line ***(ps.metrics.porosity(im))*** is to check the porosity level to the information from the input source description. If not, you may need to switch False and True in the above code or replace 0 with a different value. You can check the current values in the loaded image using np.unique(im).\n",
    "\n",
    "More info on np.from [file](https://numpy.org/doc/1.21/reference/generated/numpy.fromfile.html#:~:text=numpy.fromfile%20%C2%B6%20numpy.fromfile%28file%2C%20dtype%3Dfloat%2C%20count%3D-%201%2C%20sep%3D%27%27%2C%20offset%3D0%2C,as%20well%20as%20parsing%20simply%20formatted%20text%20files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.195033838\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TODO: checking size and names\n",
    "voxelsx = 1000\n",
    "voxelsy = 1000\n",
    "voxelsz = 1000\n",
    "path= 'rock_sample_Leopard'\n",
    "voxel_size = 2.25e-6\n",
    "im = np.fromfile(path+'/image_Leopard.raw', dtype=\"<i1\")\n",
    "im = np.reshape(im, (voxelsx,voxelsy,voxelsz)) # this value may need to be changed based on input image size\n",
    "\n",
    "# make sure pore space is True and solid is False\n",
    "pore_space = im == 0 # sometimes this may be 255 or some other value\n",
    "im[pore_space] = True\n",
    "im[~pore_space] = False\n",
    "print(ps.metrics.porosity(im))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing the image\n",
    "In this stage we slice the image to smaller pieces. This is to speed up the process of prediction with the AI approach and the DNS approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = im[:100,:100,:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation of the image\n",
    "Snow function is developed as part of the PoreSPY opensource package, Snow extracts pore and conns from the image.\n",
    "We then extract the pore network of the porous medium image using PoreSpy's snow2 algorithm. snow2 returns the segmented image of the porous medium as well as extracted network data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.10698866844177246,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04520916938781738,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Extracting pore and throat properties",
       "rate": null,
       "total": 73,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting pore and throat properties:   0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "snow = ps.networks.snow2(im, boundary_width=0, parallelization=None, voxel_size = voxel_size)\n",
    "regions = snow.regions\n",
    "net = snow.network\n",
    "conns = net['throat.conns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11:45:28] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> `lr` is deprecated, please use `learning_rate` instead, or use the legacy      <a href=\"file://C:\\Users\\Niloo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">optimizer.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\Niloo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py#106\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">106</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>         optimizer, e.g.,tf.keras.optimizers.legacy.Adam.                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11:45:28]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m `lr` is deprecated, please use `learning_rate` instead, or use the legacy      \u001b]8;id=238916;file://C:\\Users\\Niloo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py\u001b\\\u001b[2moptimizer.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=810027;file://C:\\Users\\Niloo\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py#106\u001b\\\u001b[2m106\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m         optimizer, e.g.,tf.keras.optimizers.legacy.Adam.                               \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = ps.networks.create_model()\n",
    "\n",
    "# Giving the path weights to .load function\n",
    "\n",
    "model.load_weights(path_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we intiate the AI prediction process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.026421785354614258,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Preparing images tensor",
       "rate": null,
       "total": 122,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing images tensor:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 22s 3s/step\n"
     ]
    }
   ],
   "source": [
    "predicted_ai = ps.networks.diffusive_size_factor_AI(\n",
    "    regions,\n",
    "    model=model,\n",
    "    g_train=g_train,\n",
    "    throat_conns=conns,\n",
    "    voxel_size = voxel_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we run the DNS(Direct Numerical Simulation)\n",
    "\n",
    "**Note:** This cell is often the longest to exute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03644919395446777,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Preparing images and DNS calculations",
       "rate": null,
       "total": 122,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce516643357430c9ef54033ea796dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing images and DNS calculations:   0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "startTime = time.time()\n",
    "predicted_dns = ps.networks.diffusive_size_factor_DNS(regions,\n",
    "                                                    throat_conns=conns,\n",
    "                                                         voxel_size = voxel_size)\n",
    "executionTime_dns = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime_dns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the region, you can use it for prediction. \n",
    "\n",
    "Once the values are predicted, you can save them ih hdf5 format for later use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot the Comparison between AI results and DNS predicted results.\n",
    "\n",
    "This helps us to understand the deviation between the two and also mesaure the accuracy and the correctness of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = np.max([predicted_ai, predicted_dns])\n",
    "plt.figure(figsize=[8,8])\n",
    "plt.plot(predicted_ai, predicted_dns, '*', [0, max_val], [0, max_val], 'r')\n",
    "plt.title(r'$R^2$ = ' + str(np.round(r2_score(predicted_dns, predicted_ai), 2)));\n",
    "plt.xlabel('AI values')\n",
    "plt.ylabel('DNS values')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
